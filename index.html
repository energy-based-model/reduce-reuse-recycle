
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}


h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}

.move-down {
    margin-top:1.2cm;
}

.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.center {
  margin-left: 10.0%;
  margin-right: 10.0%;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.column10 {
  text-align: center;
  float: left;
  width: 10%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}


.row-center {
    margin: 16px 0px 16px 0px;
    text-align: center;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title> Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="@du_yilun">
        <meta name="twitter:title" content="Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

 <body>


<div class="container">
    <div class="paper-title">
    <h1> 
        Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://yilundu.github.io/">Yilun Du<sup>1</sup></a>,
                <a href="https://conormdurkan.github.io/">Conor Durkan<sup>2</sup></a>,
                <a href="https://rstrudel.github.io/">Robin Strudel<sup>4</sup></a>,
                <a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en">Joshua B. Tenenbaum<sup>3</sup></a>,
                <a href="https://benanne.github.io/about/">Sander Dieleman<sup>2</sup></a>,
                <a href="https://scholar.google.com/citations?user=GgQ9GEkAAAAJ&hl=en">Rob Fergus<sup>2</sup></a>,
                <a href="http://www.sohldickstein.com/">Jascha Sohl-Dickstein<sup>3</sup></a>,
                <a href="https://www.stats.ox.ac.uk/~doucet/">Arnaud Doucet<sup>2</sup></a>,
                <a href="http://www.cs.toronto.edu/~wgrathwohl/">Will Grathwohl<sup>2</sup></a>,
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> MIT</span>
            <span><sup>2</sup> Google Deepmind</span>
            <span><sup>3</sup> Google Brain</span>
            <span><sup>4</sup> INRIA</span><br/>
        </div>

        <div class="affil-row">
            <div class="venue text-center"><b>arXiv 2023 </b></div>
        </div>

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <div class="paper-btn-coming-soon">
                <a class="paper-btn" href="">
                    <span class="material-icons"> code </span>
                    Code
                </a>
            </div>
        </div></div>
    </div>

    
    <!-- <section id="teaser-image">
        <center>
            <figure>
                <video class="centered" width="80%" autoplay loop muted playsinline class="video-background " >
                    <source src="assets/LION_video_v10.mp4#t=0.001" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>

        </center>
    </section>
     -->

    <section id="teaser-image">
        <center>
            <figure>
                <video class="centered" width="80%" autoplay loop muted playsinline class="video-background " >
                    <source src="assets/LION_video_v10.mp4#t=0.001" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>

        </center>
    </section>

    
    <section id="abstract"/>
        <hr>
        <h2>Abstract</h2>
        <div class="flex-row">
            <p>
            Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the use of new compositional operators and more sophisticated, Metropolis-corrected samplers. Intriguingly we find these samplers lead to notable improvements in compositional generation across a wide variety of problems such as classifier-guided ImageNet modeling and compositional text-to-image generation.
            </p>
        </div>
    </section>
    <section id="method"/>
        <hr>
        <h2>Method</h2>


        <div class="flex-row">
            <p> 
                We are interested in repurposing a diffusion for a set of different downstream tasks.
                LION focuses on learning a 3D generative model directly from geometry data without image-based training. 
                Similar to previous 3D DDMs in this setting, LION operates on point clouds. However, it is constructed as a VAE with DDMs in latent
                space. LION comprises a hierarchical latent space with a vector-valued global shape latent and another
                point-structured latent space. The latent representations are predicted with point cloud processing
                encoders, and two latent DDMs are trained in these latent spaces. Synthesis in LION proceeds by drawing 
                novel latent samples from the hierarchical latent DDMs and decoding back to the original point
                cloud space. Importantly, we also demonstrate how to augment LION with modern surface reconstruction methods to 
                synthesize smooth shapes as desired by artists. We propose two methodological contributions to enable better repurposing of diffusion models:
            </p>
            <p>
                <b>Annealed Distribution Transitions through MCMC:</b> 

                using a specified MCMC transition kernel.
                Extending LION with <a href="https://arxiv.org/abs/2106.03452"><b>Shape As Points</b></a> (SAP) geometry reconstruction
                allows us to also output smooth meshes. Fine-tuning SAP on data generated by LION's autoencoder
                reduces synthesis noise and enables us to generate high-quality geometry. LION combines (latent)
                point cloud-based modeling, ideal for DDMs, with surface reconstruction, desired by artists.
            </p>
            <p>
                <b>Energy Based Diffusion Parameterization:</b> Since LION is set up as a VAE, it can be easily adapted for different tasks without 
                retraining the latent DDMs: We can efficiently fine-tune LION's encoders on voxelized or noisy inputs,
                which a user can provide for guidance. This enables multimodal voxel-guided synthesis and shape 
                denoising. We also leverage LION's latent spaces for shape interpolation and autoencoding. Optionally
                training the DDMs conditioned on CLIP embeddings enables image- and text-driven 3D generation.
            </p>
            <p>
                Below, we demonstrate how we may repurppose diffusion models in a wide set of different manners. 
            </p>
        </div>
    </section>




        

    <section id="results">
        <hr>
        <h2>2D Distribution Composition</h2>  
            <center>
            <div class="flex-row">
                <p>First, we illustrate compositional generation when composing 2D distributions.</p>
            </div> 

            <figure>
                <a>
                    <img width="100%" src="materials/2d.png"> 
                </a>
                <p class="caption">
                    Generated point clouds and reconstructed meshes of airplanes.             
                </p> <br>
            </figure>
            </center>

        <hr>


        <h2>Object Generation</h2>  
            <div class="flex-row">
                <p>Below we show samples from a LION model that was trained on shapes from multiple ShapeNet catgories, <l>without any class-conditioning</l>. We did on purpose not use conditioning to explore LION's scalability to diverse and multimodal datasets in the unconditional setting.</p>
            </div> 

            <center>
            <figure>
                <a>
                    <img width="100%" src="materials/cube.png"> 
                </a>
                <p class="caption">
                    Generated point clouds and reconstructed meshes of airplanes.             
                </p> <br>
            </figure>
            </center>

        <hr>

        <h2>Class-Conditional Generation</h2>  
            <div class="flex-row">
                <p>Below we show samples from a LION model that was trained on shapes from multiple ShapeNet catgories, <l>without any class-conditioning</l>. We did on purpose not use conditioning to explore LION's scalability to diverse and multimodal datasets in the unconditional setting.</p>
            </div> 

            <center>
            <figure>
                <a>
                    <img width="100%" src="materials/classifier.png"> 
                </a>
                <p class="caption">
                    HMC sampling transitions enables more accurate classifier based generation of images
                </p> <br>
            </figure>
            </center>

        <hr>


        <h2>Compositional Text-to-Image Generation</h2>  
            <div class="flex-row">
                <p>Below we show samples from a LION model that was trained on shapes from multiple ShapeNet catgories, <l>without any class-conditioning</l>. We did on purpose not use conditioning to explore LION's scalability to diverse and multimodal datasets in the unconditional setting.</p>
            </div> 

            <center>
            <figure>
                <a>
                    <img width="80%" src="materials/text2img.png"> 
                </a>
                <p class="caption">
                    By composing different energy-based diffusion models we can generate complex combinations of
                    natural language descriptions.
                </p> <br>
            </figure>
            </center>
            

    </section> 




    <section id="related_projects">
        <hr>
        <h2>Related Projects</h2>  

          <br>
          Check out a list of our related papers on energy based models. A full list can be found <a href="https://energy-based-model.github.io/Energy-based-Model-MIT/">here</a>!
          <br>

        <div class="row vspace-top">
        <div class="col-sm-3">
            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="materials/related/teaser_glide.mp4" type="video/mp4">
            </video>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/">Compositional Visual Generation with Composable Diffusion Models</a>
        </div>
        <div>
            We present a method to compose different diffusion models together, drawing on the close connection of
            diffusion models with EBMs. We illustrate how compositional operators enable
            the ability to composing multiple sets of objects together as well as generate images subject to 
            complex text prompts.
        </div>
        </div>
        </div>

        <div class="row vspace-top">
        <div class="col-sm-3">
            <div class="move-down">
                <img src="materials/related/comp_cartoon.png" class="img-fluid" alt="comp_carton" style="width:100%">
            </div>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://energy-based-model.github.io/compositional-generation-inference/">Compositional Visual Generation with Energy Based Models</a>
        </div>
        <div>
            We present a set of compositional operators that enable EBMs to exhibit <b>zero-shot compositional</b> visual generation, enabling us to compose visual concepts
            (through operators of conjunction, disjunction, or negation) together in a zero-shot manner.
            Our approach enables us to generate faces given a  description
            ((Smiling AND Female) OR (NOT Smiling AND Male)) or to combine several different objects together.
        </div>
        </div>
        </div>


        <div class="row vspace-top">
        <div class="col-sm-3">
            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="materials/related/half.mp4" type="video/mp4">
            </video>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
                        <a href="https://openai.com/blog/energy-based-models/">Implicit Generation and Generalization with Energy Based Models</a>
        </div>
                We introduce a method to scale EBM training to generate high resolution images.
                We propose to utilize Langevin dynamics, initialized from random noise, to iteratively
                refine and denoise image samples. We further demonstrate unique properties of EBMs
                such as compositionality, continual learning, and robustness.
        <div>
        </div>
        </div>

    </section> 


    <section id="paper">
        <h2>Team</h2>        
        <div class="row">
            <div class="column5">
                <a href='https://yilundu.github.io/'>
                    <img  src=./materials/people/yilun3.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Yilun Du </p>
                <p class=institution>MIT</p>
            </div>

            <div class="column5">
                <a href='https://yilundu.github.io/'>
                    <img  src=./materials/people/conor.jpg class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Conor Durkan </p>
                <p class=institution>Google Deepmind</p>
            </div>

            <div class="column5">
                <a href='https://yilundu.github.io/'>
                    <img  src=./materials/people/robin.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Robin Strudel </p>
                <p class=institution>INRIA</p>
            </div>

            <div class="column5">
                <a href='https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en'>
                    <img  src=./materials/people/josh2.jpg class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Joshua Tenenbaum </p>
                <p class=institution>MIT</p>
            </div>

            <div class="column5">
                <a href="https://groups.csail.mit.edu/vision/torralbalab/">
                    <img src=./materials/people/sander.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Sander Dieleman </p>
                <p class=institution>Google Deepmind</p>
            </div>
         </div>

        <div class="row">

            <div class="column10">
            </div>

            <div class="column5">
                <a href='https://yilundu.github.io/'>
                    <img  src=./materials/people/rob.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Rob Fergus </p>
                <p class=institution>Google Deepmind</p>
            </div>

            <div class="column5">
                <a href='https://yilundu.github.io/'>
                    <img  src=./materials/people/jascha.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Jascha Sohl-Dickstein </p>
                <p class=institution>Google Brain</p>
            </div>

            <div class="column5">
                <a href='https://yilundu.github.io/'>
                    <img  src=./materials/people/arnaud.jpg class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Arnaud Doucet </p>
                <p class=institution>Google Deppmind</p>
            </div>

            <div class="column5">
                <a href='https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en'>
                    <img  src=./materials/people/will.jpg class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Will Grathwohl </p>
                <p class=institution>Google Deepmind</p>
            </div>

         </div>
    </section>
   
    <!-- <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{zeng2022lion,
            title={LION: Latent Point Diffusion Models for 3D Shape Generation},
            author={Xiaohui Zeng and Arash Vahdat and Francis Williams and Zan Gojcic and Or Litany and Sanja Fidler and Karsten Kreis},
            booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
            year={2022}
        }</code></pre>
    </section> -->

    <section>
        This webpage template was recycled from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
        <center><p><a href='https://accessibility.mit.edu/'><b>Accessibility</b></a></p></center>
    </section>
    


</div>
</body>
</html>
